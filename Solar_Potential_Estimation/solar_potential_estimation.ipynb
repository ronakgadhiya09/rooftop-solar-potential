{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Rooftop Solar Potential Estimation\n",
        "\n",
        "This notebook implements the complete pipeline for rooftop solar potential estimation from aerial imagery using deep learning.\n",
        "\n",
        "**Key Features:**\n",
        "- Rooftop segmentation using HRNet + OCR architecture\n",
        "- Individual roof identification and area calculation\n",
        "- Solar potential estimation using PVGIS API\n",
        "- Visualization of results\n",
        "\n",
        "**Author:** Ronak Gadhiya\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Setup Environment and Install Dependencies\n",
        "\n",
        "# Mount Google Drive (for saving outputs)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "%pip install torch torchvision\n",
        "%pip install yacs albumentations opencv-python matplotlib pandas pillow scikit-image\n",
        "%pip install pytorch-msssim opencv-contrib-python requests\n",
        "%pip install gdown  # For downloading datasets\n",
        "%pip install scikit-learn tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Clone or Download Project Files\n",
        "\n",
        "# Option 1: Clone from GitHub\n",
        "!git clone https://github.com/ronakgadhiya09/rooftop-solar-potential.git\n",
        "%cd rooftop-solar-potential\n",
        "\n",
        "# Option 2: If you prefer to download specific files directly\n",
        "# Uncomment and run the following code instead\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "# Create project structure\n",
        "!mkdir -p Solar_Potential_Estimation/configs\n",
        "!mkdir -p Solar_Potential_Estimation/model\n",
        "!mkdir -p Solar_Potential_Estimation/lib/config\n",
        "!mkdir -p Solar_Potential_Estimation/lib/utils\n",
        "!mkdir -p Solar_Potential_Estimation/inference\n",
        "!mkdir -p Solar_Potential_Estimation/test_crowd/images\n",
        "!mkdir -p Solar_Potential_Estimation/test_crowd/masks\n",
        "!mkdir -p Solar_Potential_Estimation/test_crowd/pred\n",
        "!mkdir -p Solar_Potential_Estimation/test_crowd/original/components\n",
        "!mkdir -p Solar_Potential_Estimation/test_crowd/original/roof_data\n",
        "!mkdir -p Solar_Potential_Estimation/test_crowd/original/total_pon\n",
        "!mkdir -p Solar_Potential_Estimation/test_crowd/original/com_bin\n",
        "!mkdir -p Solar_Potential_Estimation/i_outputs\n",
        "!mkdir -p solar_power_json\n",
        "\n",
        "# Download key files\n",
        "!gdown --id YOUR_ID_HERE -O Solar_Potential_Estimation/configs/inria_hrnet_ocr.yaml\n",
        "!gdown --id YOUR_ID_HERE -O Solar_Potential_Estimation/model/seg_hrnet_ocr.py\n",
        "!gdown --id YOUR_ID_HERE -O Solar_Potential_Estimation/lib/config/config.py\n",
        "!gdown --id YOUR_ID_HERE -O Solar_Potential_Estimation/lib/utils/utils.py\n",
        "!gdown --id YOUR_ID_HERE -O Solar_Potential_Estimation/inference/i_inference.py\n",
        "!gdown --id YOUR_ID_HERE -O Solar_Potential_Estimation/inference/i_roof_separation_with_area.py\n",
        "!gdown --id YOUR_ID_HERE -O Solar_Potential_Estimation/inference/get_solar_data.py\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Download Pre-trained Weights\n",
        "\n",
        "# Download pre-trained model weights\n",
        "!gdown --folder https://drive.google.com/drive/folders/1RmPxBfePZctk_RLSwMcZFcjqx4HxDiI7 -O Solar_Potential_Estimation/i_outputs\n",
        "\n",
        "# Check if weights were downloaded successfully\n",
        "import os\n",
        "if os.path.exists('Solar_Potential_Estimation/i_outputs/epoch_99.pth'):\n",
        "    print(\"âœ… Pre-trained weights downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download pre-trained weights\")\n",
        "    print(\"Please download manually from: https://drive.google.com/drive/folders/1RmPxBfePZctk_RLSwMcZFcjqx4HxDiI7\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Download Sample Test Image\n",
        "\n",
        "# Download a sample test image from the dataset\n",
        "# You can replace this with your own aerial image\n",
        "!cp Solar_Potential_Estimation/india_dataset/twoChannels_in/val/images/25_6.png Solar_Potential_Estimation/test_crowd/images/\n",
        "\n",
        "# Alternatively, upload your own image\n",
        "from google.colab import files\n",
        "print(\"You can also upload your own aerial image:\")\n",
        "# Uncomment the line below to upload your own image\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# If you uploaded an image, save it to the test directory\n",
        "\"\"\"\n",
        "import shutil\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f'Solar_Potential_Estimation/test_crowd/images/{filename}')\n",
        "    print(f\"Saved {filename} to test directory\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Import Required Libraries and Setup Paths\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Add project directories to path\n",
        "sys.path.append('Solar_Potential_Estimation')\n",
        "from model.seg_hrnet_ocr import get_seg_model\n",
        "from lib.config import config\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set paths\n",
        "CONFIG_PATH = 'Solar_Potential_Estimation/configs/inria_hrnet_ocr.yaml'\n",
        "CHECKPOINT_PATH = 'Solar_Potential_Estimation/i_outputs/epoch_99.pth'\n",
        "IMAGE_DIR = 'Solar_Potential_Estimation/test_crowd/images'\n",
        "OUTPUT_DIR = 'Solar_Potential_Estimation/test_crowd/pred'\n",
        "MASKS_DIR = 'Solar_Potential_Estimation/test_crowd/masks'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MASKS_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Load Model Configuration and Initialize Model\n",
        "\n",
        "# Load configuration\n",
        "cfg = config.config\n",
        "cfg.defrost()\n",
        "cfg.merge_from_file(CONFIG_PATH)\n",
        "cfg.freeze()\n",
        "\n",
        "# Initialize model\n",
        "model = get_seg_model(cfg)\n",
        "model = model.to(device)\n",
        "\n",
        "# Load pre-trained weights\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "if 'state_dict' in checkpoint:\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "else:\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Define Image Preprocessing Functions\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Preprocess an image for inference\n",
        "    \"\"\"\n",
        "    # Define transforms\n",
        "    transform = A.Compose([\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    \n",
        "    # Load image\n",
        "    image = np.array(Image.open(image_path).convert('RGB')).astype(np.uint8)\n",
        "    \n",
        "    # Apply transforms\n",
        "    transformed = transform(image=image)\n",
        "    image_tensor = transformed['image']\n",
        "    \n",
        "    # Add batch dimension\n",
        "    image_tensor = image_tensor.unsqueeze(0)\n",
        "    \n",
        "    return image_tensor, image\n",
        "\n",
        "def postprocess_output(output, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Convert model output to binary mask\n",
        "    \"\"\"\n",
        "    # Apply sigmoid and threshold\n",
        "    pred = torch.sigmoid(output)\n",
        "    pred = (pred > threshold).float()\n",
        "    \n",
        "    # Convert to numpy\n",
        "    pred = pred.cpu().numpy()\n",
        "    \n",
        "    return pred[0, 0]  # Return first image, first channel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Run Inference on Test Images\n",
        "\n",
        "# Get list of test images\n",
        "test_images = os.listdir(IMAGE_DIR)\n",
        "print(f\"Found {len(test_images)} test images\")\n",
        "\n",
        "# Process each image\n",
        "for img_name in test_images:\n",
        "    # Skip non-image files\n",
        "    if not (img_name.endswith('.png') or img_name.endswith('.jpg') or img_name.endswith('.jpeg')):\n",
        "        continue\n",
        "        \n",
        "    print(f\"Processing {img_name}...\")\n",
        "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "    \n",
        "    # Preprocess image\n",
        "    image_tensor, original_image = preprocess_image(img_path)\n",
        "    image_tensor = image_tensor.to(device)\n",
        "    \n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "    \n",
        "    # Postprocess output\n",
        "    pred_mask = postprocess_output(output)\n",
        "    \n",
        "    # Save prediction mask as numpy array\n",
        "    mask_filename = os.path.join(MASKS_DIR, img_name.replace('.png', '.npy').replace('.jpg', '.npy').replace('.jpeg', '.npy'))\n",
        "    np.save(mask_filename, pred_mask)\n",
        "    \n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    # Original image\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Prediction mask\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(pred_mask, cmap='gray')\n",
        "    plt.title('Predicted Mask')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Overlay\n",
        "    plt.subplot(1, 3, 3)\n",
        "    overlay = original_image.copy()\n",
        "    overlay_mask = np.zeros_like(original_image)\n",
        "    overlay_mask[:, :, 0] = pred_mask * 255  # Red channel\n",
        "    alpha = 0.5\n",
        "    cv2_overlay = cv2.addWeighted(original_image, 1, overlay_mask, alpha, 0)\n",
        "    plt.imshow(cv2_overlay)\n",
        "    plt.title('Overlay')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Save visualization\n",
        "    vis_filename = os.path.join(OUTPUT_DIR, img_name.replace('.png', '_.png').replace('.jpg', '_.png').replace('.jpeg', '_.png'))\n",
        "    plt.savefig(vis_filename)\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"âœ… Saved prediction for {img_name}\")\n",
        "\n",
        "print(\"Inference complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Roof Separation and Area Calculation\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('Solar_Potential_Estimation/test_crowd/original/components', exist_ok=True)\n",
        "os.makedirs('Solar_Potential_Estimation/test_crowd/original/roof_data', exist_ok=True)\n",
        "os.makedirs('Solar_Potential_Estimation/test_crowd/original/total_pon', exist_ok=True)\n",
        "os.makedirs('Solar_Potential_Estimation/test_crowd/original/com_bin', exist_ok=True)\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from skimage import measure\n",
        "from skimage.morphology import binary_dilation, disk\n",
        "\n",
        "def separate_roofs(mask_path, image_path, min_area=100):\n",
        "    \"\"\"\n",
        "    Separate individual roofs from a binary mask using connected components analysis\n",
        "    \"\"\"\n",
        "    # Load mask\n",
        "    mask = np.load(mask_path)\n",
        "    \n",
        "    # Load original image for visualization\n",
        "    original_image = cv2.imread(image_path)\n",
        "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Apply connected components analysis\n",
        "    labeled_mask = measure.label(mask, connectivity=2)\n",
        "    props = measure.regionprops(labeled_mask)\n",
        "    \n",
        "    # Filter regions by area\n",
        "    filtered_mask = np.zeros_like(labeled_mask)\n",
        "    roof_data = []\n",
        "    \n",
        "    for i, prop in enumerate(props):\n",
        "        if prop.area >= min_area:\n",
        "            # Keep this region\n",
        "            roof_id = i + 1\n",
        "            filtered_mask[labeled_mask == prop.label] = roof_id\n",
        "            \n",
        "            # Calculate area (in pixels)\n",
        "            roof_area = prop.area\n",
        "            \n",
        "            # Calculate usable area (excluding edges)\n",
        "            binary_mask = (labeled_mask == prop.label).astype(np.uint8)\n",
        "            dilated = binary_dilation(binary_mask, disk(3))\n",
        "            eroded = binary_dilation(binary_mask, disk(3))\n",
        "            edge_mask = dilated ^ eroded\n",
        "            usable_area = roof_area - np.sum(edge_mask)\n",
        "            \n",
        "            # Convert to real-world area (assuming 0.3m per pixel)\n",
        "            # This is a simplification - real-world applications would use GSD (Ground Sample Distance)\n",
        "            pixel_to_meter = 0.3  # 0.3 meters per pixel\n",
        "            real_area = roof_area * (pixel_to_meter ** 2)\n",
        "            \n",
        "            # Estimate number of solar panels (assuming 1.7mÂ² per panel)\n",
        "            panel_area = 1.7  # mÂ²\n",
        "            num_panels = int(real_area / panel_area)\n",
        "            \n",
        "            # Estimate solar potential (simplified: 250W per panel, 4 peak sun hours per day)\n",
        "            # Actual calculation would use PVGIS API with location data\n",
        "            solar_potential = num_panels * 0.25 * 4 * 365  # kWh per year\n",
        "            \n",
        "            roof_data.append({\n",
        "                'Roof_ID': roof_id,\n",
        "                'Roof_Area': roof_area,\n",
        "                'Net_Usable_Area': usable_area,\n",
        "                'Real_Area': real_area,\n",
        "                'Panels': num_panels,\n",
        "                'Solar_potential_per_year': solar_potential\n",
        "            })\n",
        "    \n",
        "    # Create visualization with colored roofs\n",
        "    vis_image = np.zeros_like(original_image)\n",
        "    total_potential = 0\n",
        "    \n",
        "    for roof in roof_data:\n",
        "        roof_id = roof['Roof_ID']\n",
        "        # Generate a random color for this roof\n",
        "        color = np.random.randint(0, 255, size=3)\n",
        "        vis_image[filtered_mask == roof_id] = color\n",
        "        \n",
        "        # Add text with roof ID\n",
        "        y, x = np.where(filtered_mask == roof_id)\n",
        "        if len(y) > 0 and len(x) > 0:\n",
        "            center_y, center_x = int(np.mean(y)), int(np.mean(x))\n",
        "            cv2.putText(vis_image, f\"{roof_id}\", (center_x, center_y), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "        \n",
        "        total_potential += roof['Solar_potential_per_year']\n",
        "    \n",
        "    return filtered_mask, vis_image, roof_data, total_potential\n",
        "\n",
        "# Process each mask\n",
        "for mask_file in os.listdir(MASKS_DIR):\n",
        "    if not mask_file.endswith('.npy'):\n",
        "        continue\n",
        "    \n",
        "    base_name = mask_file.replace('.npy', '')\n",
        "    mask_path = os.path.join(MASKS_DIR, mask_file)\n",
        "    image_path = os.path.join(IMAGE_DIR, f\"{base_name}.png\")\n",
        "    \n",
        "    if not os.path.exists(image_path):\n",
        "        # Try other extensions\n",
        "        for ext in ['.jpg', '.jpeg']:\n",
        "            alt_path = os.path.join(IMAGE_DIR, f\"{base_name}{ext}\")\n",
        "            if os.path.exists(alt_path):\n",
        "                image_path = alt_path\n",
        "                break\n",
        "    \n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"âš ï¸ Could not find original image for {mask_file}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"Processing {base_name}...\")\n",
        "    \n",
        "    # Separate roofs\n",
        "    labeled_mask, vis_image, roof_data, total_potential = separate_roofs(mask_path, image_path)\n",
        "    \n",
        "    # Save results\n",
        "    # 1. Save labeled mask\n",
        "    np.save(f\"Solar_Potential_Estimation/test_crowd/original/com_bin/{base_name}.npy\", labeled_mask)\n",
        "    \n",
        "    # 2. Save visualization\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(vis_image)\n",
        "    plt.title(f\"Identified Roofs: {base_name}\")\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f\"Solar_Potential_Estimation/test_crowd/original/components/{base_name}.png\")\n",
        "    plt.close()\n",
        "    \n",
        "    # 3. Save roof data as CSV\n",
        "    df = pd.DataFrame(roof_data)\n",
        "    df.to_csv(f\"Solar_Potential_Estimation/test_crowd/original/roof_data/{base_name}.csv\", index=False)\n",
        "    \n",
        "    # 4. Save total potential\n",
        "    with open(f\"Solar_Potential_Estimation/test_crowd/original/total_pon/{base_name}.txt\", 'w') as f:\n",
        "        f.write(f\"Total Solar Potential: {total_potential:.2f} kWh/year\")\n",
        "    \n",
        "    print(f\"âœ… Processed {base_name}\")\n",
        "    print(f\"   - Found {len(roof_data)} roofs\")\n",
        "    print(f\"   - Total solar potential: {total_potential:.2f} kWh/year\")\n",
        "\n",
        "print(\"Roof separation and area calculation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Get Solar Potential Data from PVGIS API (Optional)\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Create directory for solar power data\n",
        "os.makedirs('solar_power_json', exist_ok=True)\n",
        "\n",
        "def get_pvgis_data(lat=28.6139, lon=77.2090, year=2016):\n",
        "    \"\"\"\n",
        "    Get solar radiation data from PVGIS API\n",
        "    Default coordinates are for New Delhi, India\n",
        "    \"\"\"\n",
        "    url = f\"https://re.jrc.ec.europa.eu/api/v5_2/seriescalc?lat={lat}&lon={lon}&startyear={year}&endyear={year}&outputformat=json&pvcalculation=1&peakpower=1&loss=14\"\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Error: API returned status code {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data from PVGIS API: {e}\")\n",
        "        return None\n",
        "\n",
        "# Get solar radiation data for New Delhi, India\n",
        "print(\"Fetching solar radiation data from PVGIS API...\")\n",
        "pvgis_data = get_pvgis_data()\n",
        "\n",
        "if pvgis_data:\n",
        "    # Save data to file\n",
        "    with open('solar_power_json/solar_data.json', 'w') as f:\n",
        "        json.dump(pvgis_data, f, indent=2)\n",
        "    print(\"âœ… Solar radiation data saved to solar_power_json/solar_data.json\")\n",
        "    \n",
        "    # Extract and display monthly production\n",
        "    if 'outputs' in pvgis_data and 'monthly' in pvgis_data['outputs']:\n",
        "        monthly_data = pvgis_data['outputs']['monthly']\n",
        "        \n",
        "        # Extract month names and values\n",
        "        months = [month['month'] for month in monthly_data]\n",
        "        e_m = [month['E_m'] for month in monthly_data]  # Monthly energy production (kWh)\n",
        "        \n",
        "        # Plot monthly production\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.bar(months, e_m, color='orange')\n",
        "        plt.title('Monthly Solar Energy Production (kWh/kWp)')\n",
        "        plt.xlabel('Month')\n",
        "        plt.ylabel('Energy (kWh/kWp)')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.savefig('solar_power_json/monthly_production.png')\n",
        "        plt.show()\n",
        "        \n",
        "        # Calculate annual production\n",
        "        annual_production = sum(e_m)\n",
        "        print(f\"Annual solar energy production: {annual_production:.2f} kWh/kWp\")\n",
        "        print(\"This means for each kWp (kilowatt-peak) of installed capacity,\")\n",
        "        print(f\"you can expect approximately {annual_production:.2f} kWh of energy per year.\")\n",
        "        \n",
        "        # Calculate average daily production\n",
        "        avg_daily = annual_production / 365\n",
        "        print(f\"Average daily production: {avg_daily:.2f} kWh/kWp\")\n",
        "else:\n",
        "    print(\"âŒ Failed to fetch solar radiation data\")\n",
        "    print(\"Using default values for solar potential calculation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 11: Refine Solar Potential Calculation with PVGIS Data\n",
        "\n",
        "# Check if we have PVGIS data\n",
        "if os.path.exists('solar_power_json/solar_data.json'):\n",
        "    # Load PVGIS data\n",
        "    with open('solar_power_json/solar_data.json', 'r') as f:\n",
        "        pvgis_data = json.load(f)\n",
        "    \n",
        "    # Extract annual production per kWp\n",
        "    if 'outputs' in pvgis_data and 'totals' in pvgis_data['outputs']:\n",
        "        annual_production_per_kwp = pvgis_data['outputs']['totals']['E_y']\n",
        "        print(f\"Using PVGIS data: {annual_production_per_kwp:.2f} kWh/kWp annually\")\n",
        "    else:\n",
        "        # Default value if data structure is unexpected\n",
        "        annual_production_per_kwp = 1600  # Default for India\n",
        "        print(f\"Using default value: {annual_production_per_kwp:.2f} kWh/kWp annually\")\n",
        "else:\n",
        "    # Use default value\n",
        "    annual_production_per_kwp = 1600  # Default for India\n",
        "    print(f\"Using default value: {annual_production_per_kwp:.2f} kWh/kWp annually\")\n",
        "\n",
        "# Update solar potential calculations for all processed images\n",
        "for csv_file in os.listdir('Solar_Potential_Estimation/test_crowd/original/roof_data'):\n",
        "    if not csv_file.endswith('.csv'):\n",
        "        continue\n",
        "    \n",
        "    base_name = csv_file.replace('.csv', '')\n",
        "    csv_path = os.path.join('Solar_Potential_Estimation/test_crowd/original/roof_data', csv_file)\n",
        "    \n",
        "    print(f\"Updating solar potential for {base_name}...\")\n",
        "    \n",
        "    # Load roof data\n",
        "    df = pd.read_csv(csv_path)\n",
        "    \n",
        "    # Update solar potential calculation\n",
        "    # Assuming 250W panels (0.25 kWp)\n",
        "    df['Solar_potential_per_year'] = df['Panels'] * 0.25 * annual_production_per_kwp\n",
        "    \n",
        "    # Save updated data\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    \n",
        "    # Update total potential\n",
        "    total_potential = df['Solar_potential_per_year'].sum()\n",
        "    with open(f\"Solar_Potential_Estimation/test_crowd/original/total_pon/{base_name}.txt\", 'w') as f:\n",
        "        f.write(f\"Total Solar Potential: {total_potential:.2f} kWh/year\")\n",
        "    \n",
        "    print(f\"âœ… Updated solar potential for {base_name}: {total_potential:.2f} kWh/year\")\n",
        "\n",
        "print(\"Solar potential calculation refinement complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 12: Visualize Results\n",
        "\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Find all processed images\n",
        "processed_images = []\n",
        "for csv_file in os.listdir('Solar_Potential_Estimation/test_crowd/original/roof_data'):\n",
        "    if csv_file.endswith('.csv'):\n",
        "        base_name = csv_file.replace('.csv', '')\n",
        "        processed_images.append(base_name)\n",
        "\n",
        "# Create comprehensive visualization for each image\n",
        "for base_name in processed_images:\n",
        "    print(f\"Creating visualization for {base_name}...\")\n",
        "    \n",
        "    # Load original image\n",
        "    image_path = os.path.join(IMAGE_DIR, f\"{base_name}.png\")\n",
        "    if not os.path.exists(image_path):\n",
        "        # Try other extensions\n",
        "        for ext in ['.jpg', '.jpeg']:\n",
        "            alt_path = os.path.join(IMAGE_DIR, f\"{base_name}{ext}\")\n",
        "            if os.path.exists(alt_path):\n",
        "                image_path = alt_path\n",
        "                break\n",
        "    \n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"âš ï¸ Could not find original image for {base_name}\")\n",
        "        continue\n",
        "    \n",
        "    original_image = cv2.imread(image_path)\n",
        "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Load segmentation mask\n",
        "    mask_path = os.path.join(MASKS_DIR, f\"{base_name}.npy\")\n",
        "    mask = np.load(mask_path)\n",
        "    \n",
        "    # Load labeled mask\n",
        "    labeled_mask_path = f\"Solar_Potential_Estimation/test_crowd/original/com_bin/{base_name}.npy\"\n",
        "    labeled_mask = np.load(labeled_mask_path)\n",
        "    \n",
        "    # Load roof data\n",
        "    csv_path = f\"Solar_Potential_Estimation/test_crowd/original/roof_data/{base_name}.csv\"\n",
        "    roof_data = pd.read_csv(csv_path)\n",
        "    \n",
        "    # Load total potential\n",
        "    with open(f\"Solar_Potential_Estimation/test_crowd/original/total_pon/{base_name}.txt\", 'r') as f:\n",
        "        total_potential = f.read().strip()\n",
        "    \n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    \n",
        "    # Original image\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Segmentation mask\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.title('Rooftop Segmentation')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Labeled roofs\n",
        "    plt.subplot(2, 2, 3)\n",
        "    # Create a colormap for the labeled mask\n",
        "    cmap = plt.cm.get_cmap('tab20', len(roof_data) + 1)\n",
        "    colored_mask = np.zeros((*labeled_mask.shape, 3))\n",
        "    \n",
        "    # Create legend patches\n",
        "    patches = []\n",
        "    \n",
        "    # Color each roof and add to legend\n",
        "    for i, row in roof_data.iterrows():\n",
        "        roof_id = row['Roof_ID']\n",
        "        color = cmap(roof_id)[:3]  # Get RGB values\n",
        "        colored_mask[labeled_mask == roof_id] = color\n",
        "        \n",
        "        # Add to legend\n",
        "        area = row['Real_Area']\n",
        "        potential = row['Solar_potential_per_year']\n",
        "        patches.append(mpatches.Patch(color=color, \n",
        "                                      label=f\"ID {roof_id}: {area:.1f}mÂ² ({potential:.1f} kWh/yr)\"))\n",
        "    \n",
        "    plt.imshow(colored_mask)\n",
        "    plt.title('Individual Roofs with IDs')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Add legend with scrollbar if many roofs\n",
        "    if len(roof_data) > 10:\n",
        "        plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left', \n",
        "                   borderaxespad=0., fontsize='small')\n",
        "    else:\n",
        "        plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left', \n",
        "                   borderaxespad=0.)\n",
        "    \n",
        "    # Summary statistics\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Create a table with statistics\n",
        "    total_area = roof_data['Real_Area'].sum()\n",
        "    total_panels = roof_data['Panels'].sum()\n",
        "    total_energy = roof_data['Solar_potential_per_year'].sum()\n",
        "    \n",
        "    stats_text = f\"\"\"\n",
        "    # Solar Potential Summary: {base_name}\n",
        "    \n",
        "    ## Overall Statistics:\n",
        "    - Total number of roofs: {len(roof_data)}\n",
        "    - Total roof area: {total_area:.2f} mÂ²\n",
        "    - Estimated solar panels: {total_panels}\n",
        "    - Annual solar energy production: {total_energy:.2f} kWh\n",
        "    \n",
        "    ## Top 5 Roofs by Potential:\n",
        "    \"\"\"\n",
        "    \n",
        "    # Add top 5 roofs by potential\n",
        "    top_roofs = roof_data.sort_values('Solar_potential_per_year', ascending=False).head(5)\n",
        "    for i, row in top_roofs.iterrows():\n",
        "        stats_text += f\"- Roof ID {int(row['Roof_ID'])}: {row['Solar_potential_per_year']:.2f} kWh/year ({row['Real_Area']:.2f} mÂ²)\\n\"\n",
        "    \n",
        "    plt.text(0, 0.5, stats_text, fontsize=12, va='center', ha='left')\n",
        "    \n",
        "    # Adjust layout and save\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"Solar_Potential_Estimation/test_crowd/original/{base_name}_summary.png\", \n",
        "                bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"âœ… Created visualization for {base_name}\")\n",
        "\n",
        "print(\"Visualization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 13: Save Results to Google Drive (Optional)\n",
        "\n",
        "# Save results to Google Drive\n",
        "try:\n",
        "    # Create directory in Google Drive\n",
        "    drive_dir = \"/content/drive/MyDrive/Solar_Potential_Results\"\n",
        "    os.makedirs(drive_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy results\n",
        "    !cp -r Solar_Potential_Estimation/test_crowd/original/* {drive_dir}/\n",
        "    !cp -r Solar_Potential_Estimation/test_crowd/pred/* {drive_dir}/\n",
        "    !cp -r solar_power_json/* {drive_dir}/\n",
        "    \n",
        "    print(f\"âœ… Results saved to Google Drive: {drive_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error saving to Google Drive: {e}\")\n",
        "    print(\"You can manually copy the results from the following directories:\")\n",
        "    print(\"- Solar_Potential_Estimation/test_crowd/original/\")\n",
        "    print(\"- Solar_Potential_Estimation/test_crowd/pred/\")\n",
        "    print(\"- solar_power_json/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 14: Summary and Conclusion\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸŒž SOLAR POTENTIAL ESTIMATION COMPLETE ðŸŒž\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Count processed images\n",
        "processed_count = len([f for f in os.listdir('Solar_Potential_Estimation/test_crowd/original/roof_data') if f.endswith('.csv')])\n",
        "\n",
        "print(f\"Processed {processed_count} images\")\n",
        "\n",
        "# Calculate total potential across all images\n",
        "total_energy = 0\n",
        "total_area = 0\n",
        "total_roofs = 0\n",
        "total_panels = 0\n",
        "\n",
        "for csv_file in os.listdir('Solar_Potential_Estimation/test_crowd/original/roof_data'):\n",
        "    if csv_file.endswith('.csv'):\n",
        "        df = pd.read_csv(os.path.join('Solar_Potential_Estimation/test_crowd/original/roof_data', csv_file))\n",
        "        total_energy += df['Solar_potential_per_year'].sum()\n",
        "        total_area += df['Real_Area'].sum()\n",
        "        total_roofs += len(df)\n",
        "        total_panels += df['Panels'].sum()\n",
        "\n",
        "print(f\"\\nOverall Results:\")\n",
        "print(f\"- Total number of identified roofs: {total_roofs}\")\n",
        "print(f\"- Total roof area: {total_area:.2f} mÂ²\")\n",
        "print(f\"- Estimated solar panels: {total_panels}\")\n",
        "print(f\"- Annual solar energy production: {total_energy:.2f} kWh\")\n",
        "\n",
        "if total_panels > 0:\n",
        "    print(f\"- Average energy per panel: {total_energy/total_panels:.2f} kWh/year\")\n",
        "\n",
        "print(\"\\nResults are saved in the following directories:\")\n",
        "print(\"- Solar_Potential_Estimation/test_crowd/original/components/ - Individual roof visualizations\")\n",
        "print(\"- Solar_Potential_Estimation/test_crowd/original/roof_data/ - CSV files with detailed roof data\")\n",
        "print(\"- Solar_Potential_Estimation/test_crowd/original/total_pon/ - Total potential for each image\")\n",
        "print(\"- Solar_Potential_Estimation/test_crowd/original/ - Summary visualizations\")\n",
        "\n",
        "print(\"\\nThank you for using the Solar Potential Estimation tool!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 15: Optional - Training the Model (Advanced)\n",
        "\n",
        "# This cell uses the existing training script from the project\n",
        "# Training requires significant computational resources and time\n",
        "\n",
        "\"\"\"\n",
        "# Uncomment and run this cell if you want to train the model\n",
        "\n",
        "# Step 15.1: Setup Training Environment\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import argparse\n",
        "import datetime\n",
        "\n",
        "# Add project directories to path\n",
        "sys.path.append('Solar_Potential_Estimation')\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('progress', exist_ok=True)\n",
        "os.makedirs('i_outputs', exist_ok=True)\n",
        "\n",
        "# Step 15.2: Import the training script directly\n",
        "from Solar_Potential_Estimation.train_script.i_train_single_gpu import (\n",
        "    initialSetup, \n",
        "    HighResolutionNet, \n",
        "    load_pretrained_weights,\n",
        "    reinit_classifier,\n",
        "    INRIADataset,\n",
        "    DataLoader,\n",
        "    CosineAnnealingWarmRestarts,\n",
        "    configure_phase,\n",
        "    update_learning_rates,\n",
        "    train_epoch,\n",
        "    validation\n",
        ")\n",
        "\n",
        "# Step 15.3: Main Training Function\n",
        "def train_model():\n",
        "    # Initialize configuration\n",
        "    cfg = initialSetup()\n",
        "    \n",
        "    # Initialize Model\n",
        "    model = HighResolutionNet(cfg)\n",
        "    model = model.cuda()\n",
        "    \n",
        "    # Load Pretrained weights\n",
        "    pretrained_path = cfg.MODEL.PRETRAINED\n",
        "    load_pretrained_weights(model, pretrained_path)\n",
        "    reinit_classifier(model)\n",
        "    \n",
        "    # Initialize optimizer with parameter groups\n",
        "    param_groups = [\n",
        "        {'params': [p for n,p in model.named_parameters() if any(k in n for k in [\"stem\", \"conv1\", \"conv2\", \"bn1\", \"bn2\", \"layer1\", \"stage1\", \"transition1\", \"stage2\", \"transition2\", \"stage3\", \"transition3\", \"stage4\"])]},\n",
        "        {'params': [p for n,p in model.named_parameters() if 'boundary' in n or 'edge' in n]},\n",
        "        {'params': [p for n,p in model.named_parameters() if 'ocr' in n]},\n",
        "        {'params': [p for n,p in model.named_parameters() if 'cls_head' in n or 'aux_head' in n]}\n",
        "    ]\n",
        "    \n",
        "    optimizer = torch.optim.SGD(\n",
        "        param_groups,\n",
        "        lr=cfg.SOLVER.BASE_LR,\n",
        "        momentum=0.9,\n",
        "        weight_decay=cfg.SOLVER.WEIGHT_DECAY\n",
        "    )\n",
        "    \n",
        "    # Initialize learning rate scheduler\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=5,             # Restart every 5 epochs\n",
        "        T_mult=2,          # Double cycle length after each restart\n",
        "        eta_min=1e-5       # Minimum LR\n",
        "    )\n",
        "    \n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = INRIADataset(cfg.DATASET.ROOT, split='train')\n",
        "    val_dataset = INRIADataset(cfg.DATASET.ROOT, split='val')\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.SOLVER.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.SOLVER.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    # Create log files\n",
        "    with open(\"progress/i_accuracy.txt\", \"a\") as f:\n",
        "        f.write(f\"\\\\nDATE and TIME: {datetime.datetime.now()}\\\\n\")\n",
        "    \n",
        "    with open(\"progress/i_logger.txt\", \"a\") as l:\n",
        "        l.write(f\"\\\\nDATE and TIME: {datetime.datetime.now()}\\\\n\")\n",
        "    \n",
        "    # Training Loop\n",
        "    start_epoch = 0\n",
        "    best_iou = 0.0\n",
        "    patience = 15\n",
        "    no_improvement = 0\n",
        "    \n",
        "    # Check for existing checkpoint\n",
        "    checkpoint_path = 'i_outputs/epoch_99.pth'  # or your checkpoint file\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1  # Start from next epoch\n",
        "        best_iou = checkpoint['best_iou']\n",
        "        phase = checkpoint.get('phase', 'unknown')\n",
        "        print(f\"Resuming training from epoch {start_epoch} and phase {phase}\")\n",
        "    \n",
        "    # Training loop with phases\n",
        "    for epoch in range(start_epoch, cfg.SOLVER.MAX_EPOCHES):\n",
        "        trainable_count = sum(p.requires_grad for p in model.parameters())\n",
        "        print(f\"Epoch {epoch}: {trainable_count} trainable parameters\")\n",
        "        \n",
        "        # Configure model phases and learning rates\n",
        "        configure_phase(model, epoch, optimizer, cfg)\n",
        "        update_learning_rates(optimizer, epoch)\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\"Trainable params = {trainable_params}\\\\n\")\n",
        "        \n",
        "        # Training\n",
        "        train_metrics, train_loss = train_epoch(model, train_loader, optimizer, scheduler, epoch, cfg)\n",
        "        \n",
        "        # Validation\n",
        "        val_metrics = validation(model, val_loader, epoch, cfg.SOLVER.MAX_EPOCHES)\n",
        "        \n",
        "        # Save best model\n",
        "        current_iou = val_metrics['roof']['iou']\n",
        "        \n",
        "        # Update learning rate\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        \n",
        "        # Save checkpoint every 5 epochs\n",
        "        if ((epoch+1) % 5 == 0):\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_iou': best_iou,\n",
        "                'phase': 'phase1' if epoch < 15 else 'phase2',\n",
        "            }, f'i_outputs/epoch_{epoch}.pth')\n",
        "        \n",
        "        # Early stopping logic\n",
        "        if current_iou > best_iou:\n",
        "            best_iou = current_iou\n",
        "            no_improvement = 0\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_iou': best_iou,\n",
        "            }, 'i_outputs/best_model.pth')\n",
        "            print(f\"Saved best model with IoU: {best_iou:.4f}\")\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "            if no_improvement >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                print(f\"No improvement for {no_improvement} epochs\")\n",
        "                break\n",
        "        \n",
        "        # Logging\n",
        "        with open(\"progress/i_accuracy.txt\", \"a\") as f:\n",
        "            f.write(f\"\\\\nEpoch {epoch} Summary:\\\\n\")\n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            f.write(f\"Trainable params = {trainable_params}, LR = {optimizer.param_groups[0]['lr']}\\\\n\")\n",
        "            for channel in ['roof']:\n",
        "                f.write(f\"{channel.upper()}:\\\\n\")\n",
        "                f.write(f\"  Train Loss: {train_loss[f'main_{channel}'].item():.4f}\")\n",
        "                f.write(f\"  Val Loss: {val_metrics[channel]['loss']:.4f}\")\n",
        "                f.write(f\"  Train accuracy: {train_metrics[channel]['accuracy']:.4f}\")\n",
        "                f.write(f\"  Val accuracy: {val_metrics[channel]['accuracy']:.4f}\")\n",
        "                f.write(f\"  IoU: {val_metrics[channel]['iou']:.4f}\")\n",
        "                f.write(f\"  F1: {val_metrics[channel]['f1']:.4f}\")\n",
        "                f.write(f\"  Precision/Recall: {val_metrics[channel]['precision']:.4f}/{val_metrics[channel]['recall']:.4f} \\\\n\")\n",
        "            f.write(f\"\\\\n\\\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "    print(f\"Best IoU: {best_iou:.4f}\")\n",
        "    return model\n",
        "\n",
        "# Execute training - uncomment to run\n",
        "# model = train_model()\n",
        "\"\"\"\n",
        "\n",
        "print(\"Optional training cell is available but commented out.\")\n",
        "print(\"To train the model, uncomment the code in this cell and run it.\")\n",
        "print(\"Note: Training requires significant computational resources and time.\")\n",
        "print(\"For most users, using the pre-trained model is recommended.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 15: Optional - Training the Model (Advanced)\n",
        "\n",
        "# This cell uses the existing training script from the project\n",
        "# Training requires significant computational resources and time\n",
        "\n",
        "\"\"\"\n",
        "# Uncomment and run this cell if you want to train the model\n",
        "\n",
        "# Step 15.1: Setup Training Environment\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import argparse\n",
        "import datetime\n",
        "\n",
        "# Add project directories to path\n",
        "sys.path.append('Solar_Potential_Estimation')\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('progress', exist_ok=True)\n",
        "os.makedirs('i_outputs', exist_ok=True)\n",
        "\n",
        "# Step 15.2: Import the training script directly\n",
        "from Solar_Potential_Estimation.train_script.i_train_single_gpu import (\n",
        "    initialSetup, \n",
        "    HighResolutionNet, \n",
        "    load_pretrained_weights,\n",
        "    reinit_classifier,\n",
        "    INRIADataset,\n",
        "    DataLoader,\n",
        "    CosineAnnealingWarmRestarts,\n",
        "    configure_phase,\n",
        "    update_learning_rates,\n",
        "    train_epoch,\n",
        "    validation\n",
        ")\n",
        "\n",
        "# Step 15.3: Main Training Function\n",
        "def train_model():\n",
        "    # Initialize configuration\n",
        "    cfg = initialSetup()\n",
        "    \n",
        "    # Initialize Model\n",
        "    model = HighResolutionNet(cfg)\n",
        "    model = model.cuda()\n",
        "    \n",
        "    # Load Pretrained weights\n",
        "    pretrained_path = cfg.MODEL.PRETRAINED\n",
        "    load_pretrained_weights(model, pretrained_path)\n",
        "    reinit_classifier(model)\n",
        "    \n",
        "    # Initialize optimizer with parameter groups\n",
        "    param_groups = [\n",
        "        {'params': [p for n,p in model.named_parameters() if any(k in n for k in [\"stem\", \"conv1\", \"conv2\", \"bn1\", \"bn2\", \"layer1\", \"stage1\", \"transition1\", \"stage2\", \"transition2\", \"stage3\", \"transition3\", \"stage4\"])]},\n",
        "        {'params': [p for n,p in model.named_parameters() if 'boundary' in n or 'edge' in n]},\n",
        "        {'params': [p for n,p in model.named_parameters() if 'ocr' in n]},\n",
        "        {'params': [p for n,p in model.named_parameters() if 'cls_head' in n or 'aux_head' in n]}\n",
        "    ]\n",
        "    \n",
        "    optimizer = torch.optim.SGD(\n",
        "        param_groups,\n",
        "        lr=cfg.SOLVER.BASE_LR,\n",
        "        momentum=0.9,\n",
        "        weight_decay=cfg.SOLVER.WEIGHT_DECAY\n",
        "    )\n",
        "    \n",
        "    # Initialize learning rate scheduler\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=5,             # Restart every 5 epochs\n",
        "        T_mult=2,          # Double cycle length after each restart\n",
        "        eta_min=1e-5       # Minimum LR\n",
        "    )\n",
        "    \n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = INRIADataset(cfg.DATASET.ROOT, split='train')\n",
        "    val_dataset = INRIADataset(cfg.DATASET.ROOT, split='val')\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.SOLVER.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.SOLVER.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    # Create log files\n",
        "    with open(\"progress/i_accuracy.txt\", \"a\") as f:\n",
        "        f.write(f\"\\\\nDATE and TIME: {datetime.datetime.now()}\\\\n\")\n",
        "    \n",
        "    with open(\"progress/i_logger.txt\", \"a\") as l:\n",
        "        l.write(f\"\\\\nDATE and TIME: {datetime.datetime.now()}\\\\n\")\n",
        "    \n",
        "    # Training Loop\n",
        "    start_epoch = 0\n",
        "    best_iou = 0.0\n",
        "    patience = 15\n",
        "    no_improvement = 0\n",
        "    \n",
        "    # Check for existing checkpoint\n",
        "    checkpoint_path = 'i_outputs/epoch_99.pth'  # or your checkpoint file\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1  # Start from next epoch\n",
        "        best_iou = checkpoint['best_iou']\n",
        "        phase = checkpoint.get('phase', 'unknown')\n",
        "        print(f\"Resuming training from epoch {start_epoch} and phase {phase}\")\n",
        "    \n",
        "    # Training loop with phases\n",
        "    for epoch in range(start_epoch, cfg.SOLVER.MAX_EPOCHES):\n",
        "        trainable_count = sum(p.requires_grad for p in model.parameters())\n",
        "        print(f\"Epoch {epoch}: {trainable_count} trainable parameters\")\n",
        "        \n",
        "        # Configure model phases and learning rates\n",
        "        configure_phase(model, epoch, optimizer, cfg)\n",
        "        update_learning_rates(optimizer, epoch)\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\"Trainable params = {trainable_params}\\\\n\")\n",
        "        \n",
        "        # Training\n",
        "        train_metrics, train_loss = train_epoch(model, train_loader, optimizer, scheduler, epoch, cfg)\n",
        "        \n",
        "        # Validation\n",
        "        val_metrics = validation(model, val_loader, epoch, cfg.SOLVER.MAX_EPOCHES)\n",
        "        \n",
        "        # Save best model\n",
        "        current_iou = val_metrics['roof']['iou']\n",
        "        \n",
        "        # Update learning rate\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        \n",
        "        # Save checkpoint every 5 epochs\n",
        "        if ((epoch+1) % 5 == 0):\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_iou': best_iou,\n",
        "                'phase': 'phase1' if epoch < 15 else 'phase2',\n",
        "            }, f'i_outputs/epoch_{epoch}.pth')\n",
        "        \n",
        "        # Early stopping logic\n",
        "        if current_iou > best_iou:\n",
        "            best_iou = current_iou\n",
        "            no_improvement = 0\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_iou': best_iou,\n",
        "            }, 'i_outputs/best_model.pth')\n",
        "            print(f\"Saved best model with IoU: {best_iou:.4f}\")\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "            if no_improvement >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                print(f\"No improvement for {no_improvement} epochs\")\n",
        "                break\n",
        "        \n",
        "        # Logging\n",
        "        with open(\"progress/i_accuracy.txt\", \"a\") as f:\n",
        "            f.write(f\"\\\\nEpoch {epoch} Summary:\\\\n\")\n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            f.write(f\"Trainable params = {trainable_params}, LR = {optimizer.param_groups[0]['lr']}\\\\n\")\n",
        "            for channel in ['roof']:\n",
        "                f.write(f\"{channel.upper()}:\\\\n\")\n",
        "                f.write(f\"  Train Loss: {train_loss[f'main_{channel}'].item():.4f}\")\n",
        "                f.write(f\"  Val Loss: {val_metrics[channel]['loss']:.4f}\")\n",
        "                f.write(f\"  Train accuracy: {train_metrics[channel]['accuracy']:.4f}\")\n",
        "                f.write(f\"  Val accuracy: {val_metrics[channel]['accuracy']:.4f}\")\n",
        "                f.write(f\"  IoU: {val_metrics[channel]['iou']:.4f}\")\n",
        "                f.write(f\"  F1: {val_metrics[channel]['f1']:.4f}\")\n",
        "                f.write(f\"  Precision/Recall: {val_metrics[channel]['precision']:.4f}/{val_metrics[channel]['recall']:.4f} \\\\n\")\n",
        "            f.write(f\"\\\\n\\\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "    print(f\"Best IoU: {best_iou:.4f}\")\n",
        "    return model\n",
        "\n",
        "# Execute training - uncomment to run\n",
        "# model = train_model()\n",
        "\"\"\"\n",
        "\n",
        "print(\"Optional training cell is available but commented out.\")\n",
        "print(\"To train the model, uncomment the code in this cell and run it.\")\n",
        "print(\"Note: Training requires significant computational resources and time.\")\n",
        "print(\"For most users, using the pre-trained model is recommended.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
